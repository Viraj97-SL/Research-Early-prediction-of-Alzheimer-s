{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_-a6hHGaZal"
   },
   "source": [
    "Mount Google Drive and Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch, platform, os\n",
    "assert torch.cuda.is_available(), \"GPU not detected—check runtime type.\"\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install necessary libraries (TorchIO, tqdm for progress bars)\n",
    "!pip install torchio tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hemmqi5SaV7E"
   },
   "source": [
    " Extract Data Archive\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "import torchio as tio\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for Colab progress bars\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import video # Import video models for 3D ResNets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the BIDS archive into /content/adni_bids_raw\n",
    "# Check if the directory already exists and if it's populated to avoid re-extracting\n",
    "if not os.path.exists('/content/adni_bids_raw/ADNI_BIDS_raw') or \\\n",
    "   not os.listdir('/content/adni_bids_raw/ADNI_BIDS_raw'):\n",
    "    print(\"Extracting data... This may take a while.\")\n",
    "    !mkdir -p /content/adni_bids_raw\n",
    "    !tar -xf /content/drive/MyDrive/ADNI_project/data/adni_bids_raw.tar.gz -C /content/adni_bids_raw\n",
    "    print(\"Data extraction complete.\")\n",
    "else:\n",
    "    print(\"Data already extracted.\")\n",
    "\n",
    "# Verify a few files exist\n",
    "!find /content/adni_bids_raw -type f -iname \"*T1w.nii.gz\" | head -5\n",
    "\n",
    "#Explanation:** This block extracts your raw BIDS data from Google Drive to the Colab environment.\n",
    "#The `if` condition prevents re-extraction if you rerun the notebook and the data is already there, saving time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tg5b5dKNb9PD"
   },
   "source": [
    "Re-generate file_labels_final.csv and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Locate all T1w NIfTI files (case-insensitive) and save to a list\n",
    "# Use find directly as a shell command within Python\n",
    "t1w_filepaths = !find /content/adni_bids_raw -type f -iname \"*T1w.nii.gz\"\n",
    "with open('/content/t1w_list.txt', 'w') as f:\n",
    "    for path in t1w_filepaths:\n",
    "        f.write(path + '\\n')\n",
    "\n",
    "# 2. Rebuild df_files with subject_id and session_id\n",
    "files = [p.strip() for p in open('/content/t1w_list.txt')]\n",
    "records = []\n",
    "for p in files:\n",
    "    parts = p.split(os.sep)\n",
    "    subj = next((x for x in parts if x.startswith('sub-')), None)\n",
    "    ses  = next((x for x in parts if x.lower().startswith('ses-')), None) # Case-insensitive for session_id\n",
    "    records.append({\n",
    "        'filepath': p,\n",
    "        'subject_id': subj,\n",
    "        'session_id': ses\n",
    "    })\n",
    "df_files = pd.DataFrame(records)\n",
    "\n",
    "# 3. Filter to the baseline session ('ses-M00' case-insensitive)\n",
    "df_files_baseline = df_files[df_files['session_id'].str.lower() == 'ses-m00'].copy()\n",
    "print(f\"Baseline T1w scans (ses-M00): {len(df_files_baseline)}\")\n",
    "\n",
    "# 4. Load ADNIMERGE and prepare clinical labels\n",
    "adnimerge = pd.read_csv(\n",
    "    '/content/drive/MyDrive/ADNI_NewDS/ADNIMERGE_08Jun2025.csv',\n",
    "    low_memory=False\n",
    ")\n",
    "df_base = adnimerge[adnimerge['VISCODE'] == 'bl'].copy()\n",
    "\n",
    "# Normalize subject_id by stripping underscores and prepending 'sub-'\n",
    "df_base['subject_id'] = 'sub-' + df_base['PTID'].astype(str).str.replace('_', '')\n",
    "\n",
    "# Correct binary mapping for AD, MCI, CN, SMC\n",
    "mapping = {\n",
    "    'CN': 0,\n",
    "    'SMC': 0, # Assuming SMC (Subjective Memory Complaint) is considered Control\n",
    "    'EMCI': 1,\n",
    "    'LMCI': 1,\n",
    "    'MCI': 1,\n",
    "    'AD': 1\n",
    "}\n",
    "df_base['binary'] = df_base['DX_bl'].map(mapping)\n",
    "\n",
    "# Drop any rows where binary label couldn't be mapped (e.g., NaNs in DX_bl)\n",
    "df_base = df_base.dropna(subset=['binary']).copy()\n",
    "df_base['binary'] = df_base['binary'].astype(int)\n",
    "\n",
    "print(f\"\\nBaseline subjects after clinical mapping: {df_base['subject_id'].nunique()}\")\n",
    "print(\"Clinical baseline class distribution:\\n\", df_base['binary'].value_counts())\n",
    "\n",
    "# 5. Merge with df_files_baseline\n",
    "df_merged = pd.merge(\n",
    "    df_files_baseline,\n",
    "    df_base[['subject_id', 'binary']],\n",
    "    on='subject_id',\n",
    "    how='inner'\n",
    ")\n",
    "df_merged = df_merged.dropna(subset=['binary']).copy()\n",
    "df_merged['binary'] = df_merged['binary'].astype(int)\n",
    "\n",
    "print(f\"\\nFinal merged rows: {len(df_merged)}\")\n",
    "print(\"Final merged class counts:\\n\", df_merged['binary'].value_counts())\n",
    "\n",
    "# Save the final merged CSV\n",
    "df_merged.to_csv('/content/file_labels_final.csv', index=False)\n",
    "print(\"\\nSaved → /content/file_labels_final.csv\")\n",
    "\n",
    "# 6. Stratified Train/Val/Test Split\n",
    "df = pd.read_csv('/content/file_labels_final.csv')\n",
    "X = df # features (filepaths, subject_ids etc.)\n",
    "y = df['binary'] # labels for stratification\n",
    "\n",
    "# Define proportions: 70% train, 15% val, 15% test\n",
    "# First split: train_val (85%) vs test (15%)\n",
    "sss_test = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
    "for train_val_idx, test_idx in sss_test.split(X, y):\n",
    "    df_train_val = df.iloc[train_val_idx].copy().reset_index(drop=True)\n",
    "    df_test = df.iloc[test_idx].copy().reset_index(drop=True)\n",
    "\n",
    "# Second split: train (approx 70% of total) vs val (approx 15% of total) from train_val\n",
    "# Calculate test_size for the second split: val_size / (train_size + val_size)\n",
    "val_size_of_total = 0.15\n",
    "train_val_size_of_total = 0.85\n",
    "test_size_for_val_split = val_size_of_total / train_val_size_of_total # This gives approx 0.176\n",
    "\n",
    "sss_val = StratifiedShuffleSplit(n_splits=1, test_size=test_size_for_val_split, random_state=42)\n",
    "for train_idx, val_idx in sss_val.split(df_train_val, df_train_val['binary']):\n",
    "    df_train = df_train_val.iloc[train_idx].copy().reset_index(drop=True)\n",
    "    df_val = df_train_val.iloc[val_idx].copy().reset_index(drop=True)\n",
    "\n",
    "# Save to CSVs\n",
    "df_train.to_csv('/content/train.csv', index=False)\n",
    "df_val.to_csv('/content/val.csv', index=False)\n",
    "df_test.to_csv('/content/test.csv', index=False)\n",
    "\n",
    "print(\"\\n--- Dataset Splits ---\")\n",
    "print(f\"TRAIN ({len(df_train)} samples):\")\n",
    "print(df_train['binary'].value_counts(), '\\n')\n",
    "\n",
    "print(f\"VAL ({len(df_val)} samples):\")\n",
    "print(df_val['binary'].value_counts(), '\\n')\n",
    "\n",
    "print(f\"TEST ({len(df_test)} samples):\")\n",
    "print(df_test['binary'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_S_b6hO5cVm4"
   },
   "source": [
    "Define Dataset and DataLoader (with TorchIO and WeightedRandomSampler)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define patch_size (same as before)\n",
    "patch_size = (96, 128, 96)\n",
    "\n",
    "class VolumeDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        vol_np = nib.load(row.filepath).get_fdata().astype('float32')\n",
    "\n",
    "        # Normalize volume to [0, 1]\n",
    "        vol_np = (vol_np - vol_np.min()) / (np.ptp(vol_np) + 1e-6)\n",
    "\n",
    "        # Add channel dimension (C, D, H, W) for TorchIO\n",
    "        vol_tensor = torch.from_numpy(vol_np).unsqueeze(0)\n",
    "\n",
    "        # Create a TorchIO Subject\n",
    "        subject = tio.Subject(t1=tio.ScalarImage(tensor=vol_tensor))\n",
    "\n",
    "        if self.transform:\n",
    "            subject = self.transform(subject)\n",
    "\n",
    "        # Extract transformed volume and ensure float32\n",
    "        volume = subject.t1.data.float()\n",
    "        label = torch.tensor(row.binary, dtype=torch.long)\n",
    "        return volume, label\n",
    "\n",
    "# Define your TorchIO transforms\n",
    "train_transform = tio.Compose([\n",
    "    tio.Resample((1,1,1)), # Resample to 1mm isotropic\n",
    "    tio.Resize(patch_size), # Resize to a consistent patch size\n",
    "    tio.RandomFlip(axes=('LR',)), # Randomly flip left-right\n",
    "    tio.RandomElasticDeformation(), # Elastic deformation (adjust strength if too aggressive)\n",
    "    tio.RandomBiasField(), # Random bias field\n",
    "    tio.RandomMotion(), # Random motion artifacts\n",
    "    tio.RandomGamma(), # Random gamma adjustment for contrast\n",
    "    tio.RandomNoise(), # Add random Gaussian noise\n",
    "    # Add more if needed: RandomSpike, RandomGhosting\n",
    "])\n",
    "\n",
    "val_transform = tio.Compose([\n",
    "    tio.Resample((1,1,1)),\n",
    "    tio.Resize(patch_size),\n",
    "])\n",
    "\n",
    "# Function to create DataLoaders, including WeightedRandomSampler for train\n",
    "def make_loader(csv_file, transform, batch_size=2):\n",
    "    df_subset = pd.read_csv(csv_file)\n",
    "    dataset = VolumeDataset(csv_file, transform=transform)\n",
    "\n",
    "    if 'train' in csv_file: # Apply WeightedRandomSampler only for the training set\n",
    "        counts = df_subset['binary'].value_counts().sort_index().values\n",
    "        class_weights = 1.0 / counts\n",
    "        sample_weights = df_subset['binary'].map(lambda x: class_weights[x]).values\n",
    "        sampler = WeightedRandomSampler(\n",
    "            torch.from_numpy(sample_weights).double(), # Weights should be double\n",
    "            len(sample_weights),\n",
    "            replacement=True # Use replacement=True for sampling\n",
    "        )\n",
    "        # Shuffle should be False when using a sampler\n",
    "        return DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=2, drop_last=True)\n",
    "    else:\n",
    "        # For validation and test, no sampler, just shuffle=False (for consistent evaluation)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=False)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = make_loader('/content/train.csv', train_transform, batch_size=2)\n",
    "val_loader   = make_loader('/content/val.csv',   val_transform,   batch_size=2)\n",
    "test_loader  = make_loader('/content/test.csv',  val_transform,   batch_size=2)\n",
    "\n",
    "# Verify data shape (optional, but good for debugging)\n",
    "for x, y in train_loader:\n",
    "    print(f\"Train batch shape: {x.shape}, Label shape: {y.shape}\")\n",
    "    break\n",
    "#Explanation:**\n",
    "#    * The `VolumeDataset` class handles loading NIfTI files, normalization, and applying TorchIO transforms.\n",
    "#    * `train_transform` includes more aggressive augmentations.\n",
    "#    * `val_transform` remains simple for consistent evaluation.\n",
    "#    * `make_loader` function sets up `DataLoader` instances. It computes class weights and uses `WeightedRandomSampler` for the training loader to address class imbalance. `drop_last=True` for train_loader ensures all batches are of full size.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EejZVDYDdLuL"
   },
   "source": [
    " Implement the Improved 3D CNN (ResNet18 Backbone)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Import models directly from torchvision\n",
    "import torchvision.models as models\n",
    "# For pre-trained 3D models like R3D_18, they are often in video submodules or specific to certain versions.\n",
    "# Let's try importing directly from torchvision.models or explicitly from the video module if available.\n",
    "# The most common pre-trained 3D ResNet is often referred to as R3D_18 (ResNet-18 trained on Kinetics-400 dataset)\n",
    "\n",
    "class ResNet18_3D(nn.Module):\n",
    "    def __init__(self, num_classes=2, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        # Load pre-trained R3D_18 from torchvision.models.video\n",
    "        # Use the latest available weights\n",
    "        # As of recent torchvision versions, these are directly available under models.video\n",
    "        # The specific model is `r3d_18` for ResNet-18 3D.\n",
    "\n",
    "        # Correct way to access r3d_18 with pre-trained weights:\n",
    "        self.backbone = models.video.r3d_18(weights=models.video.R3D_18_Weights.DEFAULT) # Note: it's `r3d_18`, not `resnet18`\n",
    "\n",
    "        # Adjust the first convolutional layer for single-channel input (MRI)\n",
    "        # Original: self.backbone.stem.0 = nn.Conv3d(3, 64, ...) for `r3d_18`\n",
    "        # Access the first conv layer through `backbone.stem` for `r3d_18`\n",
    "        original_conv1 = self.backbone.stem[0]\n",
    "        self.backbone.stem[0] = nn.Conv3d(\n",
    "            1, # Change input channels from 3 to 1\n",
    "            original_conv1.out_channels,\n",
    "            kernel_size=original_conv1.kernel_size,\n",
    "            stride=original_conv1.stride,\n",
    "            padding=original_conv1.padding,\n",
    "            bias=original_conv1.bias\n",
    "        )\n",
    "\n",
    "        # Freeze backbone layers if requested\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            # Unfreeze the final classification layer for fine-tuning\n",
    "            self.backbone.fc.requires_grad = True\n",
    "\n",
    "        # Replace the final classification layer to match num_classes\n",
    "        num_ftrs = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# --- Training Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Compute class weights once based on the training dataset\n",
    "# (Ensure train_df is loaded from /content/train.csv from previous step)\n",
    "# This part assumes train_df is already defined from the previous successful execution of step 5\n",
    "# If not, add: train_df = pd.read_csv('/content/train.csv')\n",
    "train_df = pd.read_csv('/content/train.csv') # Ensure this line is present if not already.\n",
    "counts = train_df['binary'].value_counts().sort_index().values\n",
    "class_weights = torch.tensor([1.0/counts[0], 1.0/counts[1]],\n",
    "                             dtype=torch.float32,\n",
    "                             device=device)\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Set up loss, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Use a dummy model for optimizer/scheduler definition here;\n",
    "# actual model will be initialized per fold in the CV loop\n",
    "dummy_model = ResNet18_3D().to(device)\n",
    "optimizer = optim.AdamW(dummy_model.parameters(), lr=1e-5, weight_decay=1e-5) # Lower LR\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15) # T_max is max_epochs\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(loader, desc=\"Train\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def eval_loader(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader, desc=\"Eval\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8cwPiFEeTHF"
   },
   "source": [
    "Run 5-Fold Cross-Validation Ensemble Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score, f1_score\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # for heatmap convenience\n",
    "\n",
    "# Combine train+val for CV\n",
    "df_tv = pd.concat([pd.read_csv('/content/train.csv'),\n",
    "                   pd.read_csv('/content/val.csv')]).reset_index(drop=True)\n",
    "X_cv, y_cv = df_tv, df_tv['binary'].values\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_states = []\n",
    "all_fold_metrics = [] # To store metrics for each fold\n",
    "\n",
    "# Training loop parameters\n",
    "num_epochs_per_fold = 20 # Increased epochs for better convergence\n",
    "early_stopping_patience = 3 # Stop if val loss doesn't improve for this many epochs\n",
    "\n",
    "print(\"Starting 5-Fold Cross-Validation...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_cv, y_cv), 1):\n",
    "    print(f\"\\n--- Fold {fold}/5 ---\")\n",
    "    # Subset CSVs for current fold\n",
    "    X_cv.iloc[train_idx].to_csv('/content/tmp_train.csv', index=False)\n",
    "    X_cv.iloc[val_idx].to_csv('/content/tmp_val.csv', index=False)\n",
    "\n",
    "    # Rebuild loaders for the current fold\n",
    "    # Ensure batch_size is 2 or more, and num_workers\n",
    "    current_train_loader = make_loader('/content/tmp_train.csv', train_transform, batch_size=2)\n",
    "    current_val_loader = make_loader('/content/tmp_val.csv', val_transform, batch_size=2)\n",
    "\n",
    "    # Fresh model and optimizer for each fold\n",
    "    model = ResNet18_3D(num_classes=2, freeze_backbone=False).to(device) # Can try freeze_backbone=True for initial epochs\n",
    "    # Re-initialize optimizer and scheduler for each fold\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5) # Keep lower LR\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs_per_fold) # T_max should be num_epochs_per_fold\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_fold_state = None\n",
    "    fold_train_losses = []\n",
    "    fold_val_losses = []\n",
    "    fold_val_accs = []\n",
    "\n",
    "    for epoch in range(1, num_epochs_per_fold + 1):\n",
    "        train_loss = train_epoch(model, current_train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = eval_loader(model, current_val_loader, criterion, device)\n",
    "        scheduler.step() # Call scheduler.step() after each epoch\n",
    "\n",
    "        fold_train_losses.append(train_loss)\n",
    "        fold_val_losses.append(val_loss)\n",
    "        fold_val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs_per_fold}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.3%}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_fold_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}. No improvement for {early_stopping_patience} epochs.\")\n",
    "                break # Break out of inner epoch loop\n",
    "\n",
    "    # Plot training/validation curves for the current fold\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fold_train_losses, label='Train Loss')\n",
    "    plt.plot(fold_val_losses, label='Val Loss')\n",
    "    plt.title(f'Fold {fold} Loss Curves')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(fold_val_accs, label='Val Accuracy')\n",
    "    plt.title(f'Fold {fold} Validation Accuracy')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    if best_fold_state:\n",
    "        fold_states.append(best_fold_state)\n",
    "        # Evaluate the best model for this fold on its validation set to store metrics\n",
    "        model.load_state_dict(best_fold_state)\n",
    "        _, best_fold_val_acc = eval_loader(model, current_val_loader, criterion, device)\n",
    "        print(f\"→ Fold {fold} best val acc for best checkpoint: {best_fold_val_acc:.3%}\")\n",
    "        all_fold_metrics.append(best_fold_val_acc)\n",
    "    else:\n",
    "        print(f\"Warning: No best model state saved for Fold {fold}.\")\n",
    "\n",
    "print(\"\\n--- Cross-Validation Training Complete ---\")\n",
    "print(f\"Average best validation accuracy across folds: {np.mean(all_fold_metrics):.3%}\")\n",
    "\n",
    "# Ensemble on test set\n",
    "all_probs, all_labels = [], []\n",
    "model_for_inference = ResNet18_3D(num_classes=2).to(device) # A fresh model instance for inference\n",
    "\n",
    "print(\"\\n--- Evaluating Ensemble on Test Set ---\")\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_loader, desc=\"Test Ensemble\"):\n",
    "        x = x.to(device)\n",
    "        batch_probs = []\n",
    "        for fold_idx, state_dict in enumerate(fold_states):\n",
    "            model_for_inference.load_state_dict(state_dict)\n",
    "            model_for_inference.eval() # Set to eval mode\n",
    "            outputs = model_for_inference(x)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            batch_probs.append(probs)\n",
    "        all_probs.extend(np.mean(batch_probs, axis=0)) # Average probabilities across folds\n",
    "        all_labels.extend(y.numpy())\n",
    "\n",
    "# Final metrics with default 0.5 threshold\n",
    "preds = [1 if p >= 0.5 else 0 for p in all_probs]\n",
    "\n",
    "test_acc = accuracy_score(all_labels, preds)\n",
    "\n",
    "# --- CRITICAL CHANGE HERE: Explicitly set pos_label=1 ---\n",
    "test_prec, test_recall, test_f1, _ = precision_recall_fscore_support(\n",
    "    all_labels, preds, average='binary', zero_division=0, pos_label=1\n",
    ")\n",
    "test_auc = roc_auc_score(all_labels, all_probs)\n",
    "test_cm = confusion_matrix(all_labels, preds)\n",
    "\n",
    "print(f\"\\n--- Ensemble Test Set Results (0.5 Threshold) ---\")\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Precision: {test_prec:.3f}\")\n",
    "print(f\"Recall: {test_recall:.3f}\")\n",
    "print(f\"F1-Score: {test_f1:.3f}\")\n",
    "print(f\"AUC: {test_auc:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm) # <--- Added this back for numerical output\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(test_cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"CN\", \"AD/MCI\"],\n",
    "            yticklabels=[\"CN\", \"AD/MCI\"])\n",
    "plt.xlabel(\"Predicted Label\"); plt.ylabel(\"True Label\")\n",
    "plt.title(\"Ensemble Test Confusion Matrix (0.5 Threshold)\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {test_auc:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Ensemble Test ROC Curve')\n",
    "plt.legend(); plt.grid(); plt.show()\n",
    "\n",
    "# Threshold sweep to maximize F1 (from your previous note)\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "best_f1_th, best_f1_score, best_f1_acc = 0, -1, 0.5\n",
    "\n",
    "for t in thresholds:\n",
    "    preds_t = [1 if p >= t else 0 for p in all_probs]\n",
    "    f1 = f1_score(all_labels, preds_t, zero_division=0, pos_label=1) # <--- Added pos_label=1 here too\n",
    "    acc = accuracy_score(all_labels, preds_t)\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score, best_f1_th, best_f1_acc = f1, t, acc\n",
    "\n",
    "print(f\"\\n--- Ensemble Test Set Results (Optimal F1 Threshold) ---\")\n",
    "print(f\"Optimal F1 Threshold: {best_f1_th:.2f}\")\n",
    "print(f\"Optimal F1-Score: {best_f1_score:.3f}\")\n",
    "print(f\"Accuracy at Optimal F1 Threshold: {best_f1_acc:.3f}\")\n",
    "\n",
    "\n",
    "#* **Explanation:**\n",
    "#    * This is the core training and evaluation loop.\n",
    "#   * It implements the 5-fold cross-validation. For each fold:\n",
    "#        * It creates temporary `tmp_train.csv` and `tmp_val.csv` files.\n",
    "#        * Initializes a fresh `ResNet18_3D` model.\n",
    "#        * Sets up a new `AdamW` optimizer and `CosineAnnealingLR` scheduler.\n",
    "#        * Trains for `num_epochs_per_fold` with `early_stopping_patience`.\n",
    "#        * Saves the `state_dict` of the best model (based on validation loss) for that fold.\n",
    "#        * Plots the loss and accuracy curves for each fold to help you monitor performance.\n",
    "#    * After all folds are trained, it performs ensemble inference on the held-out `test_loader` by averaging the probabilities from all saved fold models.\n",
    "#    * Finally, it calculates and prints various performance metrics (Accuracy, Precision, Recall, F1, AUC, Confusion Matrix) for the ensemble on the test set.\n",
    "#    * It also includes the threshold sweep to find the optimal F1-score, as discussed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
